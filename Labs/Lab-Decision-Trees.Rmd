---
title: 'Lab: Decision Trees'
author: "Jonathan Bryan"
date: "July 9, 2018"
output: pdf_document
---
```{r echo = FALSE}
knitr::opts_chunk$set(fig.width=8, fig.height=5) 
```

### 8.3.1 Fitting Classification Trees
```{r}
library(tree)
library(ISLR)

#create binary sales variable
High = ifelse(Carseats$Sales <= 8, "No", "Yes")

#merge new High variabe with Carseats dataframe
Carseats = data.frame(Carseats,High)
```

```{r}
#decision tree modeling
tree.carseats = tree(High ~ . -Sales, data = Carseats)
summary(tree.carseats)
```

```{r}
#decision tree plotting
plot(tree.carseats)
text(tree.carseats, pretty=0)
tree.carseats
```

```{r}
#model validation
set.seed(2)
train = sample(1:nrow(Carseats), 200, replace = FALSE)
Carseats.train = Carseats[train,]
Carseats.test = Carseats[-train,]
High.test = Carseats.test$High

#re-run model
tree.carseats = tree(High ~ . -Sales, data= Carseats.train)
tree.pred = predict(tree.carseats, newdata=Carseats.test, type = "class")
table1 = table(tree.pred, High.test)
table1
(table1[1,1] + table1[2,2]) / sum(table1)
```

```{r}
#tree pruning
set.seed(3)
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)
names(cv.carseats)
cv.carseats
```

```{r}
#plot cv missclassification rate as function of tree size and cost-complexity parameter
par(mfrow=c(1,2))
plot(cv.carseats$size,
     cv.carseats$dev,
     type = "b")
plot(cv.carseats$k,
     cv.carseats$dev,
     type ="b")
```

```{r}
#tree pruning
prune.carseats = prune.misclass(tree.carseats,best=9)
plot(prune.carseats)
text(prune.carseats,pretty = 0)
```

```{r}
tree.pred = predict(prune.carseats,
                    newdata = Carseats.test,
                    type = "class")
table2 = table(tree.pred,High.test)
table2
(table2[1,1] + table2[2,2]) / sum(table2)
```

```{r}
#increase pruning size to 15
prune.carseats = prune.misclass(tree.carseats, best=15)
plot(prune.carseats)
text(prune.carseats, pretty=0)
tree.pred = predict(prune.carseats,
                    Carseats.test,
                    type = "class")
table3 = table(tree.pred, High.test)
table3
(table3[1,1] + table3[2,2]) / sum(table3)
```

### 8.3.2 Fitting Regression Trees
```{r}
#model regression tree
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston),
               nrow(Boston)/2,
               replace = FALSE)
tree.boston = tree(medv ~., Boston, subset = train)
summary(tree.boston)
```

```{r}
#plot regression tree
plot(tree.boston)
text(tree.boston,
     pretty = 0)
```

```{r}
#tree cross-validation
cv.boston = cv.tree(tree.boston)
plot(cv.boston$size,
     cv.boston$dev,
     type = "b")

```

```{r}
#tree pruning
prune.boston = prune.tree(tree.boston,
                          best = 5)
plot(prune.boston)
text(prune.boston, pretty=0)
```

```{r}
yhat = predict(tree.boston,
               newdata = Boston[-train,])
boston.test = Boston[-train, "medv"]
plot(yhat, boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
```

### 8.3.3 Bagging and Random Forests
```{r message=FALSE}
#bagged tree
library(randomForest)
set.seed(1)
bag.boston = randomForest(medv ~.,
                          data = Boston,
                          subset=train,
                          mtry = 13,
                          importance = TRUE)
bag.boston
```

```{r}
#bagged tree test set
yhat.bag = predict(bag.boston,
                   newdata = Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag - boston.test)^2)
```

```{r}
#bagged tree using only 25 trees
bag.boston = randomForest(medv ~.,
                          data = Boston,
                          subset = train,
                          mtry = 13,
                          ntree = 25)
yhat.bag = predict(bag.boston,
                   newdata = Boston[-train,])
mean((yhat.bag - boston.test)^2)
```

```{r}
#random forest
set.seed(1)
rf.boston = randomForest(medv ~., 
                         data = Boston, 
                         subset = train,
                         mtry = 6,
                         importance = TRUE)
yhat.rf = predict(rf.boston,
                  newdata = Boston[-train,])
mean((yhat.rf - boston.test)^2)
```

```{r}
#random forest inference
importance(rf.boston)
varImpPlot(rf.boston)
```

```{r message=FALSE}
library(gbm)
set.seed(1)
boost.boston = gbm(medv ~., data = Boston[train,],
                   distribution = "gaussian",
                   n.trees = 5000,
                   interaction.depth = 4)
summary(boost.boston)
```

```{r}
#partial dependence plots
#marginal effect of rm and lstat on medv after integrating out other variables
par(mfrow=c(1,2))
plot(boost.boston, i= "rm")
plot(boost.boston, i="lstat")
```

```{r}
#test MSE for boosted tree
yhat.boost = predict(boost.boston,
                     newdata = Boston[-train,],
                     n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```

```{r}
boost.boston = gbm(medv~.,
                   data = Boston[train,],
                   distribution = "gaussian",
                   n.trees = 5000,
                   interaction.depth = 4,
                   shrinkage = 0.2,
                   verbose = F)
yhat.boost = predict(boost.boston,
                     newdata = Boston[-train,],
                     n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```








