---
title: 'Lab 1: Subset Selection Methods'
author: "Jonathan Bryan"
date: "June 1, 2018"
output: pdf_document
---
```{r echo=FALSE}
knitr::opts_chunk$set(fig.width=8, fig.height=5) 
```

### 6.5.1 Best Subset Selection
```{r}
library(ISLR)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))

#omit observations with missing data
Hitters = na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
```

```{r}
library(leaps)

#best subset selection
regfit.full=regsubsets(Salary ~., Hitters)
summary(regfit.full)
```

```{r}
regfit.full = regsubsets(Salary ~., data=Hitters, nvmax=19)
reg.summary = summary(regfit.full)
names(reg.summary)
```

```{r}
#analysis of model selection criteria
round(reg.summary$rsq,3)
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables", ylab="RSS", type = "l")
plot(reg.summary$adjr2,xlab="Number of Variables", ylab="Adjusted RSq", type = "l")
which.max (reg.summary$adjr2)
points (11, reg.summary$adjr2[11], col ="red",cex =2, pch =20)
plot(reg.summary$cp ,xlab =" Number of Variables ",ylab="Cp",type="l")
which.min (reg.summary$cp)
points(10, reg.summary$cp [10], col ="red",cex =2, pch =20)
which.min(reg.summary$bic)
plot(reg.summary$bic ,xlab=" Number of Variables ",ylab=" BIC", type="l")
points (6, reg.summary$bic [6], col =" red",cex =2, pch =20)
```

```{r}
plot(regfit.full,scale="r2")
plot(regfit.full ,scale ="adjr2")
plot(regfit.full ,scale ="Cp")
plot(regfit.full ,scale ="bic")
```

```{r}
#coefficients of six variable model
coef(regfit.full,6)
```

### Forward and Backward Stepwise Selection
```{r}
regfit.fwd = regsubsets(Salary ~ ., data = Hitters, method="forward", nvmax = 19)
summary(regfit.fwd)
regfit.bwd = regsubsets(Salary ~ ., data = Hitters, method = "backward", nvmax = 19)
summary(regfit.bwd)
```

```{r}
round(coef(regfit.full,7),3)
round(coef(regfit.fwd,7),3)
round(coef(regfit.bwd,7),3)
```

```{r}
set.seed(1)
train = sample(c(TRUE,FALSE), size = nrow(Hitters), rep = TRUE)
test = (!train)

#fit training model
regfit.best = regsubsets(Salary ~ ., data = Hitters, subset = train, nvmax=19)

#validation set
test.mat = model.matrix(Salary ~ ., data = Hitters[test,])
val.errors = rep(NA,19)
for(i in 1:19){
  coefi = coef(regfit.best, id=i)
  pred = test.mat[,names(coefi)]%*%coefi
  val.errors[i] = mean((Hitters$Salary[test]-pred)^2)
}
val.errors
```

```{r}
#observe that the 10 variable model has the lowest test MSE
which.min(val.errors)
coef(regfit.best, 10)
```

```{r}
#prediction function for subsets
predict.regsubsets = function(object,newdata,id,...){
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object,id = id)
  xvars = names(coefi)
  mat[,xvars]%*%coefi
}
```


```{r}
#fit the best subset 10 variable model and observe coefficients
regfit.best = regsubsets(Salary ~., data = Hitters, nvmax = 19)
coef(regfit.best, 10)
```

```{r}
#k-fold CV for best subset
k=10
set.seed(1)
folds = sample(1:k, nrow(Hitters), replace = TRUE)
cv.errors = matrix(NA,k,19,dimnames = list(NULL, paste(1:19)))

#for each k-fold, run best subset for 1 to 19 variables
for(j in 1:k){
  best.fit = regsubsets(Salary ~., data = Hitters[folds!=j,], nvmax =19)
  for (i in 1:19){
    pred = predict.regsubsets(best.fit, Hitters[folds == j,], id=i)
    cv.errors[j,i] = mean((Hitters$Salary[folds ==j] - pred)^2)
  }
}
```

```{r}
mean.cv.errors = apply(cv.errors,2,mean)
mean.cv.errors
```

```{r}
par(mfrow = c(1,1))
plot(mean.cv.errors,type="b")
```

```{r}
reg.best = regsubsets(Salary ~., data = Hitters, nvmax =19)
coef(reg.best,11)
```
















