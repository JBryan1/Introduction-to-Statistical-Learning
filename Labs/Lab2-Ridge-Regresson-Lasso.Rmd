---
title: 'Lab 2: Ridge Regression and the Lasso'
author: "Jonathan Bryan"
date: "June 2, 2018"
output: pdf_document
---

### 6.6.1 Ridge Regression
```{r}
library(ISLR)
sum(is.na(Hitters))
Hitters = na.omit(Hitters)
sum(is.na(Hitters))
```

```{r}
x = model.matrix(Salary ~., Hitters)[,-1]
y = Hitters$Salary
```

```{r}
library(glmnet)
grid = 10^seq(10,-2,length=100)
ridge.mod = glmnet(x,y,alpha=0, lambda=grid)
```

```{r}
#Coefficent data when lambda = 11,498
dim(coef(ridge.mod))
ridge.mod$lambda[50]
round(coef(ridge.mod)[,50],3)
sqrt(sum(coef(ridge.mod)))
```

```{r}
#Coefficent data when lambda = 705
ridge.mod$lambda[60]
round(coef(ridge.mod)[,60],3)
sqrt(sum(coef(ridge.mod)[-1,60]^2))
```

```{r}
round(predict(ridge.mod ,s=50, type ="coefficients")[1:20 ,],3)
```

```{r}
set.seed(1)
train = sample(1:nrow(x), nrow(x)/2, replace = FALSE)
test = (-train)
y.test = y[test]
```

```{r}
#predicting Ridge (lambda = 4) on test set for test MSE
ridge.mod = glmnet(x[train,], y[train], alpha=0, lambda = grid, thresh=1e-12)
ridge.pred = predict(ridge.mod, s=4, newx=x[test,])
mean((ridge.pred-y.test)^2)
```

```{r}
#null test MSE using intercepts using mean of training observations
mean((mean(y[train])-y.test)^2)

#null test MSE using large lambda for Ridge regresion
ridge.pred=predict (ridge.mod ,s=1e10 ,newx=x[test ,])
mean((ridge.pred -y.test)^2)
```

```{r}
#comparing lambda = 4 Ridge regression to OLS
ridge.pred = predict(ridge.mod, s=0, y=y, x=x, newx=x[test,], exact=TRUE)
mean((ridge.pred-y.test)^2)
lm(y~x, subset=train)
predict(ridge.mod, s=0, y=y, x=x, exact=T, type="coefficients")[1:20]
```

```{r}
#optimal lambda search through cross validation
set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
```

```{r}
#MSE of lambda = 212
ridge.pred= predict(ridge.mod, s=bestlam, newx=x[test,])
mean((ridge.pred-y.test)^2)
```

```{r}
out = glmnet(x,y,alpha=0)
predict(out,type="coefficients", s=bestlam)[1:20,]
```

### 6.6.2 The Lasso
```{r}
lasso.mod = glmnet(x[train,], y[train],alpha=1, lambda=grid)
plot(lasso.mod)
```

```{r}
set.seed(1)
cv.out=cv.glmnet(x[train,], y[train],alpha=1)
plot(cv.out)
bestlam = cv.out$lambda.min
lasso.pred = predict(lasso.mod,s=bestlam,newx= x[test,])
mean((lasso.pred - y.test)^2)
```

```{r}
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef = predict(out, type="coefficients", s=bestlam)[1:20,]
lasso.coef
```

```{r}
lasso.coef[lasso.coef!=0]
```