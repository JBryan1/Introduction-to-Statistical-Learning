---
title: "Chapter 4 Excercises"
author: "Jonathan Bryan"
date: "April 20, 2018"
output: pdf_document
---

### 1.Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.

$$\begin{aligned}
p(x) &= \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}} \text{ (Logistic function)}\\
p(x) + p(x)e^{\beta_0 + \beta_1X} &= e^{\beta_0 + \beta_1X}\\
p(x) &= e^{\beta_0 + \beta_1X} p(x)e^{\beta_0 + \beta_1X}\\
p(x)& = e^{\beta_0 + \beta_1X} (1 + p(x))\\
\frac{p(x)}{(1 + p(x))} &= e^{\beta_0 + \beta_1X}\\
log(\frac{p(x)}{(1 + p(x))}) &= \beta_0 + \beta_1X \text{ (logit transformation)}
\end{aligned}$$

### 2.It was stated in the text that classifying an observation to the class for which (4.12) is largest is equivalent to classifying an observation to the class for which (4.13) is largest. Prove that this is the case. In other words, under the assumption that the observations in the kth class are drawn from a $N(\mu_k, \sigma^2)$ distribution, the Bayes' classifier assigns an observation to the class for which the discriminant function is maximized.

Given $X \sim N(\mu_k, \sigma^2)$ and that $log()$ is a monotonically increase function such that,
$$\forall x,y,\; x \leq y,\; log(x) < log(y)$$
We can conclude that, $arg_X \;max\; p_k(X) =  arg_X \;max\; log(p_k(X))$

### 3. This problem relates to the QDA model, in which the observations wwithin each class are drawn from a normal distribution with a class specific mean vector and a class specific covariance matrix. We consider the simple case where p = 1; i.e. there is only one feature. Suppose that we have K classes, and that if an observation belongs to the kth class then X comes from a one-dimensional normal distribution, $X \sim N(\mu_k, \sigma_k^2). Recall that the density function for the one-dimensional normal distribution is given in (4.11). Prove that in this case, the Bayes' classifier is not linear. Argue that it is in fact quadratic.

$$\begin{aligned}
p_k(X) &= \frac{1}{\sqrt{2\pi}\sigma_k}e^{-\frac{1}{2\sigma_k^2}(x - \mu_k)^2}\pi_k\\
log(p_k(X)) &= \delta_k(x) =  -\frac{log(2\pi)}{2} -log(\sigma_k) -\frac{1}{2\sigma_k^2}(x - \mu_k)^2 + log(\pi_k)\\
\delta_k(x) &\propto -\frac{1}{2\sigma_k^2}(x^2 - 2x\mu_k + \mu_k^2) -log(\sigma_k)  + log(\pi_k)\\
\delta_k(x) &\propto -\frac{x^2}{2\sigma_k^2} + \frac{x\mu_k}{\sigma_k^2} -\frac{\mu_k^2}{2\sigma_k^2} -log(\sigma_k)  + log(\pi_k)
\end{aligned}$$

As shown above, we find the value of $k$ that maximizes the observed $x$. We see that the $x$ term is quadratic in the final equation above and thus our classifier is quadratic.

### 4. When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when p is large. We will now investigate this curse.

### (a)